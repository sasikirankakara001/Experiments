{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment-6:\n",
    "#1.K-Means Clustering\n",
    "#2.K-Nearest Neighbour Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.K-Means Clustering:\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "x = np.array([[1,2],[1,4],[1,0],[10,2],[10,4],[10,0]])\n",
    "kmeans = KMeans(n_clusters=2,random_state=0).fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  2.],\n",
       "       [ 1.,  2.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  2.],\n",
       "       [ 1.,  2.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.K-Nearest Neighbours Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length','sepal-width','petal-length','petal-width','class']\n",
    "dataset = pd.read_csv(url,names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train=scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  3  8]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       0.73      1.00      0.84         8\n",
      " Iris-virginica       1.00      0.73      0.84        11\n",
      "\n",
      "       accuracy                           0.90        30\n",
      "      macro avg       0.91      0.91      0.89        30\n",
      "   weighted avg       0.93      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exp-7\n",
    "#Implement and demonstrate the FIND-S algorithm\n",
    "#for finding the most specific hypothesis based on a given set of training data samples. Read the training data from a .CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " The most general hypothesis : ['?','?','?','?','?','?']\n",
      "\n",
      "\n",
      " The most specific hypothesis : ['0','0','0','0','0','0']\n",
      "\n",
      "\n",
      " The Given Training Data Set \n",
      "\n",
      "['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Yes']\n",
      "['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Yes']\n",
      "['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'No']\n",
      "['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Yes']\n",
      "\n",
      " The initial value of hypothesis: \n",
      "['0', '0', '0', '0', '0', '0']\n",
      "\n",
      " Find S: Finding a Maximally Specific Hypothesis\n",
      "\n",
      " For Training Example No :0 the hypothesis is  ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same']\n",
      " For Training Example No :1 the hypothesis is  ['Sunny', 'Warm', '?', 'Strong', 'Warm', 'Same']\n",
      " For Training Example No :2 the hypothesis is  ['Sunny', 'Warm', '?', 'Strong', 'Warm', 'Same']\n",
      " For Training Example No :3 the hypothesis is  ['Sunny', 'Warm', '?', 'Strong', '?', '?']\n",
      "\n",
      " The Maximally Specific Hypothesis for a given Training Examples :\n",
      "\n",
      "['Sunny', 'Warm', '?', 'Strong', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "attributes = [['Sunny','Rainy'],\n",
    "              ['Warm','Cold'],\n",
    "              ['Normal','High'],\n",
    "              ['Strong','Weak'],\n",
    "              ['Warm','Cool'],\n",
    "              ['Same','Change']]\n",
    "\n",
    "\n",
    "num_attributes = len(attributes)\n",
    "\n",
    "\n",
    "print (\" \\n The most general hypothesis : ['?','?','?','?','?','?']\\n\")\n",
    "print (\"\\n The most specific hypothesis : ['0','0','0','0','0','0']\\n\")\n",
    "\n",
    "a = []\n",
    "print(\"\\n The Given Training Data Set \\n\")\n",
    "\n",
    "with open('C:/DSLab/ws.csv', 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    for row in reader:\n",
    "        a.append (row)\n",
    "        print(row)\n",
    "\n",
    "\n",
    "print(\"\\n The initial value of hypothesis: \")\n",
    "hypothesis = ['0'] * num_attributes\n",
    "print(hypothesis)\n",
    "\n",
    "# Comparing with First Training Example \n",
    "for j in range(0,num_attributes):\n",
    "        hypothesis[j] = a[0][j];\n",
    "\n",
    "# Comparing with Remaining Training Examples of Given Data Set\n",
    "\n",
    "print(\"\\n Find S: Finding a Maximally Specific Hypothesis\\n\")\n",
    "\n",
    "for i in range(0,len(a)):\n",
    "    if a[i][num_attributes]=='Yes':\n",
    "            for j in range(0,num_attributes):\n",
    "                if a[i][j]!=hypothesis[j]:\n",
    "                    hypothesis[j]='?'\n",
    "                else :\n",
    "                    hypothesis[j]= a[i][j] \n",
    "    print(\" For Training Example No :{0} the hypothesis is \".format(i),hypothesis)\n",
    "                \n",
    "print(\"\\n The Maximally Specific Hypothesis for a given Training Examples :\\n\")\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exp-8#Write a program to implement the na√Øve Bayesian classifier for a sample training data set stored as a .CSV file. Compute the accuracy of the classifier, considering few test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The length of the Data Set :  768\n",
      "\n",
      " The Data Set Splitting into Training and Testing \n",
      "\n",
      "\n",
      " Number of Rows in Training Set:514 rows\n",
      "\n",
      " Number of Rows in Testing Set:254 rows\n",
      "\n",
      " First Five Rows of Training Set:\n",
      "\n",
      "[5.0, 95.0, 72.0, 33.0, 0.0, 37.7, 0.37, 27.0, 0.0] \n",
      "\n",
      "[5.0, 88.0, 66.0, 21.0, 23.0, 24.4, 0.342, 30.0, 0.0] \n",
      "\n",
      "[2.0, 118.0, 80.0, 0.0, 0.0, 42.9, 0.693, 21.0, 1.0] \n",
      "\n",
      "[7.0, 106.0, 60.0, 24.0, 0.0, 26.5, 0.296, 29.0, 1.0] \n",
      "\n",
      "[1.0, 163.0, 72.0, 0.0, 0.0, 39.0, 1.222, 33.0, 1.0] \n",
      "\n",
      "\n",
      " First Five Rows of Testing Set:\n",
      "\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0] \n",
      "\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0, 0.0] \n",
      "\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0, 1.0] \n",
      "\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0, 0.0] \n",
      "\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0, 1.0] \n",
      "\n",
      "\n",
      " Model Summaries:\n",
      " {0.0: [(3.2928994082840237, 2.945601236194455), (111.09467455621302, 27.75918969467676), (67.83727810650888, 17.64890446376465), (20.0207100591716, 14.805450018022407), (73.8224852071006, 106.487866559384), (30.36420118343195, 7.842620533835833), (0.4358786982248519, 0.30192305515856593), (31.50887573964497, 12.170703913378427)], 1.0: [(4.732954545454546, 3.672639271801576), (142.9090909090909, 31.51675703536095), (70.76136363636364, 21.692655409177057), (21.8125, 18.046101676381127), (99.10227272727273, 146.57105851703886), (35.19545454545455, 7.348436709799035), (0.5334261363636366, 0.3563755245706234), (36.42613636363637, 11.468974240526926)]}\n",
      "\n",
      "Predictions:\n",
      " [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]\n",
      "\n",
      " Accuracy: 76.77165354330708%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "# 1.Data Handling\n",
    "# 1.1 Loading the Data from csv file of Pima indians diabetes dataset.\n",
    "def loadcsv(filename):\n",
    "    lines = csv.reader(open(filename, \"r\"))\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        # converting the attributes from string to floating point numbers\n",
    "        dataset[i] = [float(x) for x in dataset[i]] \n",
    "    return dataset\n",
    "\n",
    "#1.2 Splitting the Data set into Training Set\n",
    "def splitDataset(dataset, splitRatio):\n",
    "    trainSize = int(len(dataset) * splitRatio)\n",
    "    trainSet = []\n",
    "    copy = list(dataset)\n",
    "    while len(trainSet) < trainSize:\n",
    "        index = random.randrange(len(copy)) # random index\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [trainSet, copy]\n",
    " \n",
    "#2.Summarize Data\n",
    "#The naive bayes model is comprised of a \n",
    "#summary of the data in the training dataset. \n",
    "#This summary is then used when making predictions.\n",
    "#involves the mean and the standard deviation for each attribute, by class value\n",
    "\n",
    "#2.1: Separate Data By Class\n",
    "#Function to categorize the dataset in terms of classes \n",
    "#The function assumes that the last attribute (-1) is the class value. \n",
    "#The function returns a map of class values to lists of data instances.\n",
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated\n",
    " \n",
    "#The mean is the central middle or central tendency of the data, \n",
    "# and we will use it as the middle of our gaussian distribution \n",
    "# when calculating probabilities\n",
    "\n",
    "#2.2 : Calculate Mean\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "#The standard deviation describes the variation of spread of the data, \n",
    "#and we will use it to characterize the expected spread of each attribute\n",
    "#in our Gaussian distribution when calculating probabilities.\n",
    "\n",
    "#2.3 : Calculate Standard Deviation\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)\n",
    " \n",
    "#2.4 : Summarize Dataset\n",
    "#Summarize Data Set for a list of instances (for a class value) \n",
    "#The zip function groups the values for each attribute across our data instances \n",
    "#into their own lists so that we can compute the mean and standard deviation values \n",
    "#for the attribute.\n",
    "\n",
    "def summarize(dataset):\n",
    "    summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "    del summaries[-1]\n",
    "    return summaries\n",
    " \n",
    "#2.5 : Summarize Attributes By Class\n",
    "#We can pull it all together by first separating our training dataset into \n",
    "#instances grouped by class.Then calculate the summaries for each attribute.\n",
    "\n",
    "def summarizeByClass(dataset):\n",
    "    separated = separateByClass(dataset)\n",
    "    summaries = {}\n",
    "    for classValue, instances in separated.items():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries\n",
    "\n",
    "#3.Make Prediction\n",
    "#3.1 Calculate Probaility Density Function\n",
    "def calculateProbability(x, mean, stdev):\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n",
    "\n",
    "#3.2 Calculate Class Probabilities\n",
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "    probabilities = {}\n",
    "    for classValue, classSummaries in summaries.items():\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(classSummaries)):\n",
    "            mean, stdev = classSummaries[i]\n",
    "            x = inputVector[i]\n",
    "            probabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "    return probabilities\n",
    "\n",
    "#3.3 Prediction : look for the largest probability and return the associated class\n",
    "def predict(summaries, inputVector):\n",
    "    probabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    return bestLabel\n",
    " \n",
    "#4.Make Predictions\n",
    "# Function which return predictions for list of predictions\n",
    "# For each instance\n",
    "\n",
    "def getPredictions(summaries, testSet):\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = predict(summaries, testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "#5. Computing Accuracy \n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i][-1] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    " \n",
    "#Main Function  \n",
    "def main():\n",
    "    filename = 'C:/DSLab/pima-indians.csv'\n",
    "    splitRatio = 0.67\n",
    "    dataset = loadcsv(filename)\n",
    "    \n",
    "    #print(\"\\n The Data Set :\\n\",dataset)\n",
    "    print(\"\\n The length of the Data Set : \",len(dataset))\n",
    "    \n",
    "    print(\"\\n The Data Set Splitting into Training and Testing \\n\")\n",
    "    trainingSet, testSet = splitDataset(dataset, splitRatio)\n",
    "    \n",
    "    print('\\n Number of Rows in Training Set:{0} rows'.format(len(trainingSet)))\n",
    "    print('\\n Number of Rows in Testing Set:{0} rows'.format(len(testSet)))\n",
    "    \n",
    "    print(\"\\n First Five Rows of Training Set:\\n\")\n",
    "    for i in range(0,5):\n",
    "        print(trainingSet[i],\"\\n\")\n",
    "    \n",
    "    print(\"\\n First Five Rows of Testing Set:\\n\")\n",
    "    for i in range(0,5):\n",
    "        print(testSet[i],\"\\n\")\n",
    "   \n",
    "    # prepare model\n",
    "    summaries = summarizeByClass(trainingSet)\n",
    "    print(\"\\n Model Summaries:\\n\",summaries)\n",
    "   \n",
    "    # test model\n",
    "    predictions = getPredictions(summaries, testSet)\n",
    "    print(\"\\nPredictions:\\n\",predictions)\n",
    "    \n",
    "    accuracy = getAccuracy(testSet, predictions)\n",
    "    print('\\n Accuracy: {0}%'.format(accuracy))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exp9#Assuming a set of documents that need to be classified, use the na√Øve Bayesian Classifier model to perform this task. Built-in Java classes/API can be used to write the program. Calculate the accuracy, precision, and recall for your data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the dataset (18, 2)\n",
      "0                      I love this sandwich\n",
      "1                  This is an amazing place\n",
      "2        I feel very good about these beers\n",
      "3                      This is my best work\n",
      "4                      What an awesome view\n",
      "5             I do not like this restaurant\n",
      "6                  I am tired of this stuff\n",
      "7                    I can't deal with this\n",
      "8                      He is my sworn enemy\n",
      "9                       My boss is horrible\n",
      "10                 This is an awesome place\n",
      "11    I do not like the taste of this juice\n",
      "12                          I love to dance\n",
      "13        I am sick and tired of this place\n",
      "14                     What a great holiday\n",
      "15           That is a bad locality to stay\n",
      "16           We will have good fun tomorrow\n",
      "17         I went to my enemy's house today\n",
      "Name: message, dtype: object\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    1\n",
      "11    0\n",
      "12    1\n",
      "13    0\n",
      "14    1\n",
      "15    0\n",
      "16    1\n",
      "17    0\n",
      "Name: labelnum, dtype: int64\n",
      "(5,)\n",
      "(13,)\n",
      "(5,)\n",
      "(13,)\n",
      "Accuracy metrics\n",
      "Accuracy of the classifer is 0.6\n",
      "Confusion matrix\n",
      "[[1 1]\n",
      " [1 2]]\n",
      "Recall and Precison \n",
      "0.6666666666666666\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 112 from C header, got 124 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "msg=pd.read_csv('C:/DSLab/naivetext1.csv',names=['message','label'])\n",
    "print('The dimensions of the dataset',msg.shape)\n",
    "msg['labelnum']=msg.label.map({'pos':1,'neg':0})\n",
    "X=msg.message\n",
    "y=msg.labelnum\n",
    "print(X)\n",
    "print(y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,y)\n",
    "print(xtest.shape)\n",
    "print(xtrain.shape)\n",
    "print(ytest.shape)\n",
    "print(ytrain.shape)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "xtrain_dtm = count_vect.fit_transform(xtrain)\n",
    "xtest_dtm=count_vect.transform(xtest)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(xtrain_dtm,ytrain)\n",
    "predicted = clf.predict(xtest_dtm)\n",
    "from sklearn import metrics\n",
    "print('Accuracy metrics')\n",
    "print('Accuracy of the classifer is',metrics.accuracy_score(ytest,predicted))\n",
    "print('Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,predicted))\n",
    "print('Recall and Precison ')\n",
    "print(metrics.recall_score(ytest,predicted))\n",
    "print(metrics.precision_score(ytest,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
